"use strict";(globalThis.webpackChunkai_book_frontend=globalThis.webpackChunkai_book_frontend||[]).push([[1113],{854(n,i,e){e.r(i),e.d(i,{assets:()=>a,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module4/week12-decision-making","title":"Decision-Making for Robots","description":"Understanding how Physical AI systems make decisions in complex, dynamic environments","source":"@site/docs/module4/week12-decision-making.md","sourceDirName":"module4","slug":"/module4/week12-decision-making","permalink":"/docs/module4/week12-decision-making","draft":false,"unlisted":false,"editUrl":"https://github.com/ishaq-niazi/ai-book/edit/main/docs/module4/week12-decision-making.md","tags":[{"inline":true,"label":"decision-making","permalink":"/docs/tags/decision-making"},{"inline":true,"label":"planning","permalink":"/docs/tags/planning"},{"inline":true,"label":"autonomy","permalink":"/docs/tags/autonomy"},{"inline":true,"label":"ai","permalink":"/docs/tags/ai"},{"inline":true,"label":"robotics","permalink":"/docs/tags/robotics"}],"version":"current","sidebarPosition":16,"frontMatter":{"title":"Decision-Making for Robots","description":"Understanding how Physical AI systems make decisions in complex, dynamic environments","sidebar_position":16,"tags":["decision-making","planning","autonomy","ai","robotics"]},"sidebar":"tutorialSidebar","previous":{"title":"Kinematics & Movement","permalink":"/docs/module4/week11-kinematics-movement"},"next":{"title":"Full System Overview","permalink":"/docs/module4/week13-full-system-overview"}}');var r=e(2540),l=e(3023);const t={title:"Decision-Making for Robots",description:"Understanding how Physical AI systems make decisions in complex, dynamic environments",sidebar_position:16,tags:["decision-making","planning","autonomy","ai","robotics"]},o="Decision-Making for Robots",a={},c=[{value:"Introduction to Decision-Making in Physical AI",id:"introduction-to-decision-making-in-physical-ai",level:2},{value:"Fundamentals of Robot Decision-Making",id:"fundamentals-of-robot-decision-making",level:2},{value:"Decision-Making Hierarchy",id:"decision-making-hierarchy",level:3},{value:"Decision-Making Framework",id:"decision-making-framework",level:3},{value:"Classical Decision-Making Approaches",id:"classical-decision-making-approaches",level:2},{value:"Rule-Based Systems",id:"rule-based-systems",level:3},{value:"Search-Based Planning",id:"search-based-planning",level:3},{value:"Planning and Decision-Making",id:"planning-and-decision-making",level:2},{value:"Classical Planning",id:"classical-planning",level:3},{value:"Probabilistic Planning",id:"probabilistic-planning",level:3},{value:"Learning-Based Decision-Making",id:"learning-based-decision-making",level:2},{value:"Reinforcement Learning",id:"reinforcement-learning",level:3},{value:"Supervised Learning for Decision-Making",id:"supervised-learning-for-decision-making",level:3},{value:"Multi-Objective Decision-Making",id:"multi-objective-decision-making",level:2},{value:"Utility Theory",id:"utility-theory",level:3},{value:"Multi-Criteria Decision Analysis",id:"multi-criteria-decision-analysis",level:3},{value:"Uncertainty and Risk Management",id:"uncertainty-and-risk-management",level:2},{value:"Probabilistic Reasoning",id:"probabilistic-reasoning",level:3},{value:"Risk Assessment",id:"risk-assessment",level:3},{value:"Real-Time Decision-Making",id:"real-time-decision-making",level:2},{value:"Anytime Algorithms",id:"anytime-algorithms",level:3},{value:"Computational Efficiency",id:"computational-efficiency",level:3},{value:"Human-Robot Decision-Making",id:"human-robot-decision-making",level:2},{value:"Shared Autonomy",id:"shared-autonomy",level:3},{value:"Social Decision-Making",id:"social-decision-making",level:3},{value:"Applications in Physical AI",id:"applications-in-physical-ai",level:2},{value:"Autonomous Vehicles",id:"autonomous-vehicles",level:3},{value:"Service Robotics",id:"service-robotics",level:3},{value:"Industrial Robotics",id:"industrial-robotics",level:3},{value:"Search and Rescue",id:"search-and-rescue",level:3},{value:"Challenges and Limitations",id:"challenges-and-limitations",level:2},{value:"Computational Complexity",id:"computational-complexity",level:3},{value:"Uncertainty and Robustness",id:"uncertainty-and-robustness",level:3},{value:"Safety and Ethics",id:"safety-and-ethics",level:3},{value:"Future Directions",id:"future-directions",level:2},{value:"Advanced Decision-Making Techniques",id:"advanced-decision-making-techniques",level:3},{value:"Integration with Other Capabilities",id:"integration-with-other-capabilities",level:3}];function d(n){const i={h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,l.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(i.header,{children:(0,r.jsx)(i.h1,{id:"decision-making-for-robots",children:"Decision-Making for Robots"})}),"\n",(0,r.jsx)(i.h2,{id:"introduction-to-decision-making-in-physical-ai",children:"Introduction to Decision-Making in Physical AI"}),"\n",(0,r.jsx)(i.p,{children:"Decision-making represents the cognitive capabilities that enable Physical AI systems to choose appropriate actions based on their current state, environmental information, and goals. Unlike traditional computer programs that follow predetermined sequences, Physical AI systems must make decisions in real-time while navigating uncertainty, incomplete information, and competing objectives."}),"\n",(0,r.jsx)(i.p,{children:"Effective decision-making in Physical AI systems requires balancing multiple considerations: safety, efficiency, task completion, and adaptability to changing conditions. The challenge lies in creating systems that can make good decisions quickly enough to operate in real-world environments while handling the complexity and uncertainty inherent in physical systems."}),"\n",(0,r.jsx)(i.h2,{id:"fundamentals-of-robot-decision-making",children:"Fundamentals of Robot Decision-Making"}),"\n",(0,r.jsx)(i.h3,{id:"decision-making-hierarchy",children:"Decision-Making Hierarchy"}),"\n",(0,r.jsx)(i.p,{children:"Robot decision-making typically operates at multiple levels of abstraction:"}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Executive Level"}),": High-level goal management and mission planning"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Goal specification"}),": Defining and prioritizing objectives"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Mission planning"}),": Sequencing multiple tasks and objectives"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Resource allocation"}),": Managing computational and physical resources"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Temporal reasoning"}),": Planning over extended time periods"]}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Task Level"}),": Specific task planning and execution"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Task decomposition"}),": Breaking complex tasks into manageable components"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Action selection"}),": Choosing appropriate actions for specific situations"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Constraint satisfaction"}),": Respecting operational and environmental constraints"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Performance optimization"}),": Balancing competing objectives"]}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Reactive Level"}),": Immediate response to environmental changes"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Event detection"}),": Identifying significant environmental changes"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Immediate response"}),": Rapid reaction to critical situations"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Safety management"}),": Ensuring safe operation in all conditions"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Exception handling"}),": Managing unexpected situations"]}),"\n"]}),"\n",(0,r.jsx)(i.h3,{id:"decision-making-framework",children:"Decision-Making Framework"}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Perception-Decision-Action Cycle"}),": The fundamental decision-making loop"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Sensing"}),": Gathering information about the environment and internal state"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"State estimation"}),": Understanding current situation and context"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Goal evaluation"}),": Assessing progress toward objectives"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Action selection"}),": Choosing the best action for current situation"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Execution"}),": Implementing the selected action"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Monitoring"}),": Observing results and adapting as needed"]}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Uncertainty Management"}),": Handling incomplete and noisy information"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Probabilistic reasoning"}),": Using probability to represent uncertainty"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Belief state"}),": Maintaining estimates of system and environment state"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Information gathering"}),": Deciding when to seek additional information"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Robust decision-making"}),": Making decisions that work despite uncertainty"]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"classical-decision-making-approaches",children:"Classical Decision-Making Approaches"}),"\n",(0,r.jsx)(i.h3,{id:"rule-based-systems",children:"Rule-Based Systems"}),"\n",(0,r.jsx)(i.p,{children:"Rule-based systems use predefined if-then rules to make decisions:"}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Production Systems"}),": Collections of condition-action rules"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Rule structure"}),": If condition then action format"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Conflict resolution"}),": Methods for choosing between competing rules"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Forward chaining"}),": Applying rules to known facts"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Backward chaining"}),": Working backwards from goals"]}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Expert Systems"}),": Rule-based systems encoding human expertise"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Knowledge engineering"}),": Extracting and encoding expert knowledge"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Inference engines"}),": Mechanisms for applying rules"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Explanation capability"}),": Understanding why decisions were made"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Maintenance"}),": Updating rules as knowledge evolves"]}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Behavior-Based Systems"}),": Predefined behaviors for different situations"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Behavior design"}),": Creating specialized behaviors for specific tasks"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Behavior arbitration"}),": Choosing which behavior to execute"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Subsumption architecture"}),": Higher behaviors overriding lower ones"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Reactive control"}),": Immediate response to environmental changes"]}),"\n"]}),"\n",(0,r.jsx)(i.h3,{id:"search-based-planning",children:"Search-Based Planning"}),"\n",(0,r.jsx)(i.p,{children:"Search-based approaches explore possible action sequences:"}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"State Space Search"}),": Exploring possible system states"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"State representation"}),": Mathematical description of system configuration"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Action models"}),": Understanding how actions change the state"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Goal testing"}),": Determining when the goal has been reached"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Search strategies"}),": Methods for exploring the state space"]}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Action Space Search"}),": Exploring sequences of actions"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Action selection"}),": Choosing which actions to consider"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Plan evaluation"}),": Assessing the quality of action sequences"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Pruning strategies"}),": Eliminating unpromising action sequences"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Optimality guarantees"}),": Ensuring optimal or near-optimal solutions"]}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Heuristic Search"}),": Using domain knowledge to guide search"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Heuristic functions"}),": Estimating distance to goal"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Admissible heuristics"}),": Guaranteed to find optimal solutions"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Consistent heuristics"}),": Maintaining consistency across search"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Efficiency gains"}),": Reducing search space with good heuristics"]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"planning-and-decision-making",children:"Planning and Decision-Making"}),"\n",(0,r.jsx)(i.h3,{id:"classical-planning",children:"Classical Planning"}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"STRIPS Planning"}),": State-Transition Representation for Action Planning"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"State representation"}),": Propositional logic for state description"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Action representation"}),": Precondition and effect models"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Plan generation"}),": Finding action sequences to achieve goals"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Planning algorithms"}),": Methods for generating plans efficiently"]}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Hierarchical Task Networks (HTN)"}),": Planning with high-level tasks"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Task decomposition"}),": Breaking complex tasks into subtasks"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Method schemas"}),": Procedures for accomplishing tasks"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Temporal reasoning"}),": Planning with temporal constraints"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Resource reasoning"}),": Managing resource usage in plans"]}),"\n"]}),"\n",(0,r.jsx)(i.h3,{id:"probabilistic-planning",children:"Probabilistic Planning"}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Markov Decision Processes (MDP)"}),": Sequential decision-making under uncertainty"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"States"}),": Possible system configurations"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Actions"}),": Available choices in each state"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Transition probabilities"}),": Likelihood of state changes"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Rewards"}),": Values assigned to state-action pairs"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Optimal policy"}),": Best action for each state"]}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Partially Observable MDPs (POMDP)"}),": Planning with incomplete information"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Observations"}),": Information received about the state"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Belief states"}),": Probability distributions over possible states"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Policy optimization"}),": Decisions based on belief state"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Computational complexity"}),": Challenges in solving POMDPs"]}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Monte Carlo Methods"}),": Simulation-based decision-making"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Sampling"}),": Using random samples to estimate values"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Rollout algorithms"}),": Simulating possible futures"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Upper Confidence Bounds"}),": Balancing exploration and exploitation"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Real-time application"}),": Efficient methods for online decision-making"]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"learning-based-decision-making",children:"Learning-Based Decision-Making"}),"\n",(0,r.jsx)(i.h3,{id:"reinforcement-learning",children:"Reinforcement Learning"}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Basic Concepts"}),": Learning through interaction and reward"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Environment interaction"}),": Agent takes actions, receives feedback"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Reward signal"}),": Feedback about action quality"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Policy learning"}),": Learning what actions to take"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Value estimation"}),": Learning how good states are"]}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Value-Based Methods"}),": Learning the value of states and actions"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Q-Learning"}),": Learning action-value functions"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Deep Q-Networks"}),": Using neural networks for value estimation"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Experience replay"}),": Learning from past experiences"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Target networks"}),": Stabilizing learning with fixed targets"]}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Policy-Based Methods"}),": Directly learning action selection policies"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Policy gradients"}),": Optimizing policies directly"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Actor-critic methods"}),": Combining value and policy learning"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Trust region methods"}),": Stable policy improvement"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Model-free vs. model-based"}),": Learning with or without environment models"]}),"\n"]}),"\n",(0,r.jsx)(i.h3,{id:"supervised-learning-for-decision-making",children:"Supervised Learning for Decision-Making"}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Imitation Learning"}),": Learning from expert demonstrations"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Behavior cloning"}),": Imitating demonstrated actions"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Dataset aggregation"}),": Iteratively improving with new data"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Inverse reinforcement learning"}),": Learning reward functions from demonstrations"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Generalization"}),": Applying learned behaviors to new situations"]}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Predictive Models"}),": Learning to predict system behavior"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Dynamics models"}),": Predicting how actions affect the system"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Environment models"}),": Predicting environmental changes"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Uncertainty modeling"}),": Predicting prediction uncertainty"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Model-based planning"}),": Using learned models for planning"]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"multi-objective-decision-making",children:"Multi-Objective Decision-Making"}),"\n",(0,r.jsx)(i.h3,{id:"utility-theory",children:"Utility Theory"}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Utility Functions"}),": Quantifying preferences over outcomes"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Preference ordering"}),": Ranking different outcomes"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Utility measurement"}),": Assigning numerical values to preferences"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Multi-attribute utility"}),": Combining multiple objectives"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Certainty equivalence"}),": Trading off risk and reward"]}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Pareto Optimality"}),": Solutions that can't improve one objective without harming another"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Pareto frontier"}),": Set of optimal trade-off solutions"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Weighted sum methods"}),": Combining objectives with weights"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Constraint methods"}),": Optimizing one objective while constraining others"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Epsilon-constraint methods"}),": Systematic exploration of trade-offs"]}),"\n"]}),"\n",(0,r.jsx)(i.h3,{id:"multi-criteria-decision-analysis",children:"Multi-Criteria Decision Analysis"}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Analytic Hierarchy Process"}),": Structured decision-making with multiple criteria"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Hierarchical decomposition"}),": Breaking decisions into criteria"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Pairwise comparison"}),": Comparing criteria and alternatives"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Consistency checking"}),": Ensuring rational comparisons"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Synthesis"}),": Combining comparisons into overall priorities"]}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"TOPSIS"}),": Technique for Order Preference by Similarity to Ideal Solution"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Normalized decision matrix"}),": Scaling different criteria"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Ideal solutions"}),": Best and worst possible solutions"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Distance calculation"}),": Distance from ideal solutions"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Ranking"}),": Ordering alternatives by similarity to ideal"]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"uncertainty-and-risk-management",children:"Uncertainty and Risk Management"}),"\n",(0,r.jsx)(i.h3,{id:"probabilistic-reasoning",children:"Probabilistic Reasoning"}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Bayesian Networks"}),": Representing probabilistic relationships"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Graphical models"}),": Directed graphs representing dependencies"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Conditional probability tables"}),": Quantifying relationships"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Inference algorithms"}),": Computing probabilities from evidence"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Learning"}),": Learning structure and parameters from data"]}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Fuzzy Logic"}),": Handling imprecise and vague information"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Fuzzy sets"}),": Sets with gradual membership"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Fuzzy rules"}),": Rules with fuzzy conditions and actions"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Defuzzification"}),": Converting fuzzy outputs to crisp decisions"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Applications"}),": Control systems with imprecise requirements"]}),"\n"]}),"\n",(0,r.jsx)(i.h3,{id:"risk-assessment",children:"Risk Assessment"}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Risk Modeling"}),": Quantifying potential negative outcomes"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Probability assessment"}),": Likelihood of different outcomes"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Consequence analysis"}),": Impact of different outcomes"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Risk metrics"}),": Measures for quantifying risk"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Risk tolerance"}),": Acceptable levels of risk"]}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Decision Trees"}),": Modeling sequential decisions under uncertainty"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Chance nodes"}),": Representing uncertain outcomes"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Decision nodes"}),": Representing choice points"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Value nodes"}),": Representing outcome values"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Backward induction"}),": Computing optimal decisions from leaves"]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"real-time-decision-making",children:"Real-Time Decision-Making"}),"\n",(0,r.jsx)(i.h3,{id:"anytime-algorithms",children:"Anytime Algorithms"}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Anytime Planning"}),": Providing solutions that improve over time"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Initial solution"}),": Quickly providing feasible solution"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Iterative refinement"}),": Improving solution with additional time"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Interruptibility"}),": Returning best solution when interrupted"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Quality-time trade-offs"}),": Balancing solution quality and computation time"]}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Receding Horizon Control"}),": Planning over finite time windows"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Predictive models"}),": Predicting system behavior"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Optimization horizon"}),": Planning over finite time window"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Feedback correction"}),": Adjusting plans based on system response"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Stability guarantees"}),": Ensuring stable system behavior"]}),"\n"]}),"\n",(0,r.jsx)(i.h3,{id:"computational-efficiency",children:"Computational Efficiency"}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Algorithm Optimization"}),": Improving computational performance"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Pruning strategies"}),": Eliminating unpromising alternatives"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Heuristic search"}),": Guiding search with domain knowledge"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Approximation methods"}),": Trading accuracy for speed"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Parallel processing"}),": Exploiting computational parallelism"]}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Hierarchical Decision-Making"}),": Breaking decisions into levels"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Coarse-to-fine"}),": Making decisions at different resolution levels"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Level coordination"}),": Ensuring consistency across levels"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Efficiency gains"}),": Reducing computation through hierarchy"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Solution quality"}),": Maintaining quality with hierarchical approach"]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"human-robot-decision-making",children:"Human-Robot Decision-Making"}),"\n",(0,r.jsx)(i.h3,{id:"shared-autonomy",children:"Shared Autonomy"}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Human-Robot Collaboration"}),": Combining human and robot decision-making"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Complementary capabilities"}),": Leveraging different strengths"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Authority sharing"}),": Determining who decides what"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Communication"}),": Sharing information between human and robot"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Trust building"}),": Establishing and maintaining trust"]}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Teachable Agents"}),": Allowing humans to guide robot decision-making"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Instruction giving"}),": Humans providing guidance"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Learning from feedback"}),": Improving with human input"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Explanation capability"}),": Understanding robot decision-making"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Adaptation"}),": Adjusting to human preferences"]}),"\n"]}),"\n",(0,r.jsx)(i.h3,{id:"social-decision-making",children:"Social Decision-Making"}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Social Conventions"}),": Following human social norms"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Etiquette"}),": Following social protocols"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Turn-taking"}),": Managing interaction timing"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Personal space"}),": Respecting human comfort zones"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Cultural sensitivity"}),": Adapting to cultural differences"]}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Group Decision-Making"}),": Multiple agents making decisions together"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Consensus algorithms"}),": Reaching agreement among agents"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Voting systems"}),": Collecting preferences from multiple agents"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Negotiation protocols"}),": Resolving conflicts between agents"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Coordination mechanisms"}),": Ensuring consistent group behavior"]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"applications-in-physical-ai",children:"Applications in Physical AI"}),"\n",(0,r.jsx)(i.h3,{id:"autonomous-vehicles",children:"Autonomous Vehicles"}),"\n",(0,r.jsx)(i.p,{children:"Decision-making in transportation systems:"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Traffic navigation"}),": Deciding when to change lanes, stop, or proceed"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Risk assessment"}),": Evaluating and managing collision risks"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Route planning"}),": Choosing optimal paths considering multiple factors"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Emergency response"}),": Making rapid decisions in critical situations"]}),"\n"]}),"\n",(0,r.jsx)(i.h3,{id:"service-robotics",children:"Service Robotics"}),"\n",(0,r.jsx)(i.p,{children:"Decision-making in service applications:"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Task prioritization"}),": Managing multiple simultaneous requests"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Resource allocation"}),": Efficiently using time and energy"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Human interaction"}),": Deciding how to interact with humans"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Adaptive behavior"}),": Adjusting to changing user needs"]}),"\n"]}),"\n",(0,r.jsx)(i.h3,{id:"industrial-robotics",children:"Industrial Robotics"}),"\n",(0,r.jsx)(i.p,{children:"Decision-making in manufacturing:"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Production optimization"}),": Maximizing efficiency and quality"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Quality control"}),": Deciding when products meet standards"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Maintenance scheduling"}),": Determining when to perform maintenance"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Safety management"}),": Ensuring safe operation in human environments"]}),"\n"]}),"\n",(0,r.jsx)(i.h3,{id:"search-and-rescue",children:"Search and Rescue"}),"\n",(0,r.jsx)(i.p,{children:"Decision-making in emergency response:"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Mission planning"}),": Prioritizing search areas and objectives"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Risk management"}),": Balancing mission success with safety"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Resource coordination"}),": Managing multiple robots and humans"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Real-time adaptation"}),": Adjusting plans based on new information"]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"challenges-and-limitations",children:"Challenges and Limitations"}),"\n",(0,r.jsx)(i.h3,{id:"computational-complexity",children:"Computational Complexity"}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Real-time requirements"}),": Decisions must be made quickly"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Algorithm efficiency"}),": Optimizing computation for speed"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Hardware acceleration"}),": Using specialized processors"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Approximation methods"}),": Trading optimality for speed"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Parallel processing"}),": Exploiting computational parallelism"]}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Scalability"}),": Systems must work for large, complex problems"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"State space explosion"}),": Exponential growth in complexity"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Planning horizon"}),": Balancing long-term and short-term planning"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Multi-agent coordination"}),": Managing decisions across multiple agents"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Communication overhead"}),": Sharing information between agents"]}),"\n"]}),"\n",(0,r.jsx)(i.h3,{id:"uncertainty-and-robustness",children:"Uncertainty and Robustness"}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Model uncertainty"}),": Real systems don't match models"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Model learning"}),": Improving models through experience"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Robust decision-making"}),": Making decisions that work despite model errors"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Adaptive control"}),": Adjusting decisions as models change"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Verification"}),": Ensuring decisions remain safe and effective"]}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Environmental uncertainty"}),": Changing conditions affecting decisions"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Online adaptation"}),": Adjusting decisions based on new information"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Risk management"}),": Planning for uncertain outcomes"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Contingency planning"}),": Preparing for unexpected situations"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Resilience"}),": Maintaining functionality despite uncertainty"]}),"\n"]}),"\n",(0,r.jsx)(i.h3,{id:"safety-and-ethics",children:"Safety and Ethics"}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Safety-critical decisions"}),": Ensuring safe operation"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Fail-safe mechanisms"}),": Ensuring safe states during failures"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Safety verification"}),": Proving safety properties of decision-making"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Emergency procedures"}),": Predefined responses to critical situations"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Human oversight"}),": Maintaining human control when needed"]}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Ethical decision-making"}),": Making decisions consistent with values"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Value alignment"}),": Ensuring robot values align with human values"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Transparency"}),": Understanding how decisions are made"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Accountability"}),": Determining responsibility for decisions"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Privacy"}),": Respecting human privacy in decision-making"]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"future-directions",children:"Future Directions"}),"\n",(0,r.jsx)(i.h3,{id:"advanced-decision-making-techniques",children:"Advanced Decision-Making Techniques"}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Neural Decision-Making"}),": Using neural networks for complex decisions"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Deep reinforcement learning"}),": Learning complex decision policies"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Neural planning"}),": Learning planning algorithms with neural networks"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"End-to-end learning"}),": Learning from perception to action"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Generalization"}),": Applying learned decisions to new situations"]}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Human-AI Collaboration"}),": Advanced human-robot decision-making"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Natural interaction"}),": Making decisions through natural communication"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Intention recognition"}),": Understanding human goals and intentions"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Trust calibration"}),": Maintaining appropriate levels of trust"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Explainable AI"}),": Understanding and explaining robot decisions"]}),"\n"]}),"\n",(0,r.jsx)(i.h3,{id:"integration-with-other-capabilities",children:"Integration with Other Capabilities"}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Perception-Action Integration"}),": Combining sensing with decision-making"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Active perception"}),": Deciding what to sense based on needs"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Closed-loop control"}),": Integrating perception and action decisions"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Attention mechanisms"}),": Focusing on relevant information"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Information bottleneck"}),": Managing information flow efficiently"]}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Learning and Adaptation"}),": Improving decision-making through experience"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Continual learning"}),": Learning without forgetting previous knowledge"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Transfer learning"}),": Applying knowledge to new domains"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Meta-learning"}),": Learning how to learn new decision-making skills"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Self-improvement"}),": Systems that improve their own decision-making"]}),"\n"]}),"\n",(0,r.jsx)(i.p,{children:"Understanding decision-making provides the foundation for creating Physical AI systems that can operate autonomously and effectively in complex, dynamic environments, making intelligent choices that balance multiple objectives and constraints."})]})}function h(n={}){const{wrapper:i}={...(0,l.R)(),...n.components};return i?(0,r.jsx)(i,{...n,children:(0,r.jsx)(d,{...n})}):d(n)}},3023(n,i,e){e.d(i,{R:()=>t,x:()=>o});var s=e(3696);const r={},l=s.createContext(r);function t(n){const i=s.useContext(l);return s.useMemo(function(){return"function"==typeof n?n(i):{...i,...n}},[i,n])}function o(n){let i;return i=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:t(n.components),s.createElement(l.Provider,{value:i},n.children)}}}]);